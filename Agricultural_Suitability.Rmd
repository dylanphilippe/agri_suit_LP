---
title: "Agri_Suit"
author: "Dylan Philippe & Alexander Lehner"
date: "10/04/2024"
output: html_document
---



Clear workplace and memory
```{r}
rm(list = ls())
gc()
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "SET WORKING DIRECTORY HERE") # Set the knit root.dir here
```



```{r, echo=F}
# We import the necessary packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(raster, tmap, dplyr, sf, R.utils, ggplot2, rgeos, 
               nngeo, exactextractr, gifski, rlist, ncdf4, tidyr, zoo, 
               fasterize, minpack.lm)

options(scipen = 999) # This option allows to remove the scientific notation

# Deactivate the spherical geometry S2
sf::sf_use_s2(FALSE)

CRSproject <- "EPSG:4326"
EXTproject <- c(-25, 40, 35, 70) # See bellow explanation of the choice of the extent c(xmin, xmax, ymin, ymax)
```

# Part 1: Gathering different Climate surface and Soil data 

## Creation of the grid
We will create a grid covering the extent of the analysis. Using a grid rather than keeping the raster will allow us to avoid the raster distortion has we are currently using raster with different resolution and extent.
```{r}
# We first load the world shapefile from Natural Earth
world <- st_read("./inputs/Admin_units/ne_50m_land.shp")
world <- st_transform(world, CRSproject)

world

# We now create a bounding box around our area of interest (europe) and that correspond to the max extent of the different rasters 
my_box <- rgeos::bbox2SP(n = 70, s = 35, w = -25, e = 40,
                         proj4string = CRS("+proj=longlat +datum=WGS84 +no_defs +type=crs"))

my_box_sf <- st_as_sf(st_make_grid(my_box, n = c(1,1)))

# We intersect the two shapefile to get only the part of europe we are interested in 
europe <- st_intersection(world, my_box_sf)
europe

tm_shape(europe) + tm_polygons()
# Following most of the raster resolution, we can create a grid with 0.5 degree resolution.
my_rast <- raster(extent(EXTproject), res = 0.5, crs = CRSproject)
grid_europe <- rasterToPolygons(my_rast)
grid_europe <- st_as_sf(grid_europe)

# We create the grid identifier
grid_europe$gid <- seq(1, nrow(grid_europe), by = 1)
my_rast
# Remove the "layer" variable
grid_europe <- grid_europe %>% dplyr::select(-layer)

# We dave the grid of europe and border for later use
 save(grid_europe, file = "./processed_data/grid_europe.Rdata", overwrite = T)
 save(europe, file = "./processed_data/europe.Rdata", overwrite = T)

# We remove temporary files 
rm(my_box, my_rast, my_box_sf, world, grid)

tmap_leaflet(tm_shape(grid_europe) + tm_polygons(alpha = 0)) # We have now our grid of Europe
```


## CRU dataset
We import and process the climate surface data from the Climate Research Unit (CRU)
```{r}
# the function need 4 inputs downloaded directly from  the CRU: sunshine, humidity, windspeed and elevation.
# In addition to those 4 climate surfaces, I build a raster indicating the latitude. This raster will be necessary for further computations.
# The output of the function is a list of 5 element: 1/ average sunshine hours 2/ average humidity (%) 3/ average wind speed (m/s) 4/ median elevation 5/ latitude raster (degree)

source("code/[fun]CRU_manipulation.R")
List_CRU <- CRU_manipulation("inputs/CRU_CL_v.2.0/grid_10min_sunp.dat.gz", "inputs/CRU_CL_v.2.0/grid_10min_reh.dat.gz", "inputs/CRU_CL_v.2.0/grid_10min_wnd.dat.gz", 
                             "inputs/CRU_CL_v.2.0/grid_10min_elv.dat.gz")


# The raster in the list are covering the world, but we are only interested in a part of europe, we then have to change the extent of the rasters
for (i in 1:5){
  List_CRU[[i]] <- crop(List_CRU[[i]], EXTproject)
}

# We can rename the element in the list
names(List_CRU) <- c("sun", "hum", "win", "lat", "elev")
```


Extraction at grid level
```{r}
# We duplicate the grid so that we always keep the grid_europe as a raw file for further extraction 
grid_CRU <- grid_europe

# We extract mean value fo climate surface to the grid
grid_CRU$sun <- exact_extract(List_CRU[["sun"]], grid_CRU, fun = "mean")
grid_CRU$hum <- exact_extract(List_CRU[["hum"]], grid_CRU, fun = "mean")
grid_CRU$win <- exact_extract(List_CRU[["win"]], grid_CRU, fun = "mean")
grid_CRU$lat <- exact_extract(List_CRU[["lat"]], grid_CRU, fun = "mean")
grid_CRU$elev <- exact_extract(List_CRU[["elev"]], grid_CRU, fun = "mean")

tm_shape(grid_CRU) + tm_polygons("sun") + tm_shape(europe) + tm_borders()

# Save grid
 save(grid_CRU, file = "./processed_data/grid_CRU.Rdata", overwrite = F)
```

## Temperatures 
We will now import the temperature and precipitations dataset by Luterbacher and Pauling 
```{r Temperature manipulation}
# Let's first import the historical temperature dataset from Luterbacher.

# The goal of the function is to create 3 different stacks of rasters indicating the minimum, maximum and mean temperature for each year in the dataset.
source("code/[fun]temp_data_manipulation.R")

temp_mean_min_max_1500_2002 <- temp_data_manipulation("inputs/Temperature/TT_Europe_1500_2002.txt")

# Since we are not interested in years after 2000, we will only keep 501 layers; 1500 to 2000 ( without 2001 and 2002)
for (i in 1:3){
temp_mean_min_max_1500_2002[[i]] <- dropLayer(temp_mean_min_max_1500_2002[[i]], c(502, 503))
}

# We rename the dataset and drop the old one 
temp_mean_min_max_1500_2000 <- temp_mean_min_max_1500_2002
remove(temp_mean_min_max_1500_2002)
```


Extraction at grid level
```{r}
grid_mean_temp <- rep(list(grid_europe), 501)
grid_min_temp <- rep(list(grid_europe), 501)
grid_max_temp <- rep(list(grid_europe), 501)

# Create a sequence of years to paste the temperature and precipitations
years <- seq(1500, 2000, by = 1) %>% as.character()

# We start with mean temperature 
for (i in 1:501){
  print(i)
  grid_mean_temp[[i]]$tmp <- exact_extract(temp_mean_min_max_1500_2000[[1]][[i]], grid_europe, fun = "mean")
  colnames(grid_mean_temp[[i]])[colnames(grid_mean_temp[[i]]) == 'tmp'] <- paste("tmp_", years[i], sep = "")
}

# Save grid 
 save(grid_mean_temp, file = "./processed_data/grid_mean_temp.Rdata", overwrite = F)

# We start with mean temperature 
for (i in 1:501){
   print(i)
  grid_min_temp[[i]]$tmn <- exact_extract(temp_mean_min_max_1500_2000[[2]][[i]], grid_europe, fun = "mean")
  colnames(grid_min_temp[[i]])[colnames(grid_min_temp[[i]]) == 'tmn'] <- paste("tmn_", years[i], sep = "")
}

# Save grid
 save(grid_min_temp, file = "./processed_data/grid_min_temp.Rdata", overwrite = F)

# Max temperatures 
for (i in 1:501){
   print(i)
  grid_max_temp[[i]]$tmx <- exact_extract(temp_mean_min_max_1500_2000[[3]][[i]], grid_europe, fun = "mean")
  colnames(grid_max_temp[[i]])[colnames(grid_max_temp[[i]]) == 'tmx'] <- paste("tmx_", years[i], sep = "")
}

# Save grid
 save(grid_max_temp, file = "./processed_data/grid_max_temp.Rdata", overwrite = F)
```


## Precipitation 
```{r Precipitation manipulation}
# We turn now to the precipitation dataset by Pauling et al.

source("code/[fun]pre_data_manipulation.R")

# The function takes as input the 4 files indicating the observations for the 4 seasons as follow; fun(autumn, winter, spring, summer). The output will be 500 stacks of rasters indicating yearly mean precipitations (in mm).
pre_1500_2000 <- pre_data_manipulation("inputs/Precipitation/prec-pauling-au.txt", "inputs/Precipitation/prec-pauling-wi.txt", "inputs/Precipitation/prec-pauling-sp.txt", 
                                       "inputs/Precipitation/prec-pauling-su.txt")

```

Extracting at grid level 
```{r}
# Create the list where we will store the data
grid_pre <- rep(list(grid_europe), 501)

# Create the year sequence
years <- as.character(seq(1500, 2000, by = 1))

for (i in 1:501){
  print(i)
  grid_pre[[i]]$pre <- exact_extract(pre_1500_2000[[i]], grid_europe, fun = "mean")
  colnames(grid_pre[[i]])[colnames(grid_pre[[i]]) == 'pre'] <- paste("pre_", years[i], sep = "")
}

tm_shape(grid_pre[[1]]) + tm_polygons("pre_1500") + tm_shape(europe) + tm_borders()

save(grid_pre, file = "./processed_data/grid_pre.Rdata", overwrite = T)
```


## Evapotranspiration 

The calculation procedure of those parameters has been outsourced but need the following inputs: Mean temperature, max temperature, min temperature, wind speed at 2m in m/s, relative humidity, potential sushine hours per day, latitude, elevation and the Julian data.
```{r, echo=FALSE, warning=FALSE}
# We load the different climate dataset 
load(file = "./processed_data/grid_CRU.Rdata")
load(file = "./processed_data/grid_max_temp.Rdata")
load(file = "./processed_data/grid_mean_temp.Rdata")
load(file = "./processed_data/grid_min_temp.Rdata")
load(file = "./processed_data/grid_pre.Rdata")

source("code/[fun]evapo_grid.R")

# the function needs the following inputs: Reference_Evapo(A, B, C, D, E, F, G , H)
# A: Temperature raster
# B: Max temperature raster 
# C: Min temperature raster
# D: Wind speed raster
# E: Relative humidity raster
# F: Sunshine hours per day raster
# G: Latitude raster
# H: Elevation raster

# Time invariant stuff
WIND <- grid_CRU %>% st_drop_geometry() %>% dplyr::select(win) %>%
  unlist() %>% as.numeric()
HUM <- grid_CRU %>% st_drop_geometry() %>% dplyr::select(hum) %>%
  unlist() %>% as.numeric()
SUN <- grid_CRU %>% st_drop_geometry() %>% dplyr::select(sun) %>%
  unlist() %>% as.numeric()
LATITUDE <- grid_CRU %>% st_drop_geometry() %>% dplyr::select(lat) %>%
  unlist() %>% as.numeric()
ELEVATION <- grid_CRU %>% st_drop_geometry() %>% dplyr::select(elev) %>%
  unlist() %>% as.numeric()

# The grid where we will store the yearly ET0
ET0_year <- rep(list(0), 501)

for (j in 1:501){
  # Counter so that we can check the progress
   print(j)
TEMPER <- grid_mean_temp[[j]] %>% st_drop_geometry() %>% dplyr::select(-gid) %>%
  unlist() %>% as.numeric()

TMAX <- grid_max_temp[[j]] %>% st_drop_geometry() %>% dplyr::select(-gid) %>% 
  unlist() %>% as.numeric()

TMIN <- grid_min_temp[[j]] %>% st_drop_geometry() %>% dplyr::select(-gid) %>% 
  unlist() %>% as.numeric()

# Parameters
Tau <- rep(list(0), 365)
delta <- rep(list(0), 365)
d <- rep(list(0), 365)
psi <- rep(list(0), 365)
Ra <- rep(list(0), 365)
DL <- rep(list(0), 365)
Rs <- rep(list(0), 365)
Rns <- rep(list(0), 365)
Rnl <- rep(list(0), 365)
Rn <- rep(list(0), 365)
ETra <- rep(list(0), 365)
ET0 <- rep(list(0), 365)
ET0_grid <- rep(list(grid_europe), 365)

# We use the function 
ET0_bind <- Reference_Evapo(TEMPER, TMAX, TMIN, WIND, HUM, SUN, LATITUDE, ELEVATION)

# Since everything is binded together, we can just summarize by grouping by grid id
ET0_year[[j]] <- ET0_bind %>% group_by(gid) %>% summarise(ET0 = mean(ET0, na.rm = T)) 
ET0_year
}

Reference_Evapo_grid <- rep(list(grid_europe), 501)

for (i in 1:501){
  Reference_Evapo_grid[[i]] <- left_join(Reference_Evapo_grid[[i]], ET0_year[[i]], by = "gid")
}

# Save the dataset 
 save(Reference_Evapo_grid, file = "./processed_data/grid_evapo.Rdata", overwrite = T)
```

## Aridity Index
The next chunk takes care of the Aridity index which is simple the ratio between annual precipitations and reference evapotranspiration.

```{r}
load(file = "./processed_data/grid_evapo.Rdata")
load(file = "./processed_data/grid_pre.Rdata")

AI_index <- rep(list(0), 501)
grid_AI <- rep(list(grid_europe), 501)

for (i in 1:501){
 AI_index[[i]] <- grid_pre[[i]] %>% st_drop_geometry() %>% dplyr::select(-gid)/Reference_Evapo_grid[[i]]  %>% st_drop_geometry() %>% dplyr::select(-gid) %>% mutate(ET0 = ET0*365)

# We change the names 
colnames(AI_index[[i]]) <- "AI"

# We bring the grid id
gid <- grid_europe$gid %>% as.numeric()

AI_index[[i]]$gid <- gid

grid_AI[[i]] <- left_join(grid_AI[[i]], AI_index[[i]], by = "gid")
}

 save(grid_AI, file = "./processed_data/grid_AI.Rdata", overwrite = T)
```


## HWSD
We process data form the harmonized world soil database

The carbon content from the HWSD
source: https://daac.ornl.gov/SOILS/guides/HWSD.html

```{r}
# Top soil carbon content 
carbon_HWSD <- raster("inputs/HWSD/T_C.nc4")

# We extract the carbon content at grid level 
grid_HWSD <- grid_europe

grid_HWSD$carbon <- exact_extract(carbon_HWSD, grid_HWSD, fun = "mean")

#We pursue our analysis using now the pH content in top soil
#Top soil pH content (in H2O) in -log(H+)

# We import the soil pH 
pH <- raster("inputs/HWSD/T_PH_H2O.nc4")

# We extract it at grid level 
grid_HWSD$pH <- exact_extract(pH, grid_HWSD, fun = "mean")

# Save the grids
save(grid_HWSD, file = "./processed_data/grid_HWSD.Rdata", overwrite = T)
```


## GDD
Here we import the 4 seasons. Keeping the four distinct seasons will allow to gain more precision in the computation of the Growing Degree Days
```{r}
  historical_temp <- as.matrix(read.table("inputs/Temperature/TT_Europe_1500_2002.txt", sep = ""))
  # The table is huge. We can see that the first column give the year and the season.
  # Hence, the first line given by 150013 is the average temperature for winter 1500.
  # We can henceforth create a loop over the lines to extract all the year and season and generate rasters out of it.
  dim(historical_temp)
  
  # We have 2012 lines and 9101 columns
  # We can first drop the first column as we know when it starts and when it ends
  historical_temp <- historical_temp[, -1]
  dim(historical_temp)
  
  # We have now 9100 column which correspond to the number of gridpoints. Let's create the loop
  # We now create the loop over each rows and store every matrix in a list
  mydata <- rep(list(0), 2012)
  
  for (i in 1:2012){
    mydata[[i]] <- matrix(historical_temp[i, ], nrow = 70, ncol = 130, byrow = T)
  }
  
  # We can now split the list and group it in 4 so that we have 503 lists with 4 raster inside: 1 for each season
  mydata <- split(mydata, rep(1:ceiling(2012/4), each = 4)[1:2012])
  
  # The seasons are: [[1]] Winter [[2]] Spring [[3]] Summer [[4]] Autumn for each of the 503 elements in the list
  # Remov 2001 and 2002 so that we have our 501 obs
  mydata <- mydata[-c(502, 503)]
```


```{r, echo=F}
# For the GDD we will use the seasonal temperatures to gain precision and avoid the strict 0 for north/Est Europe in case the mean temperature is bellow 5°C
# Hence, we construct 4 measures of GDD, one for each season. This will be then averaged over each year
# Since each season sums up to 3 months, we have approximately 91 days in total (365/4 = 91.25)

# Little function that computes GDD for each season
fun_GDD <- function(tmp){
  tmp[tmp == -999.99] <- NA
  tmp <- tmp - 5
  tmp[tmp < 0] <- 0
  GDD <- tmp*91.25
}


GDDlist <- rep(list(0), 501)

for (i in 1:501){
  GDDlist[[i]] <- (fun_GDD(mydata[[i]][[1]]) + fun_GDD(mydata[[i]][[2]]) + fun_GDD(mydata[[i]][[3]]) + fun_GDD(mydata[[i]][[4]]))
  GDDlist[[i]] <- raster(GDDlist[[i]], crs = CRSproject)
  extent(GDDlist[[i]]) <- EXTproject
}

# Extract at grid level
grid_GDD <- rep(list(grid_europe), 501)

for (i in 1:501){
  print(i)
  grid_GDD[[i]]$gdd <- exact_extract(GDDlist[[i]], grid_GDD[[i]], fun = "mean")
}

 save(grid_GDD, file = "./processed_data/grid_GDD.Rdata", overwrite = T)
```

# Part 2: Model Calibration

## A) Fitting the parameters
We calibrate our model using the FAO GAEZ suitability maps.

```{r}
grid_FAO <- grid_europe
# Downloaded from FAO GAEZ data portal
wheat <- raster("./inputs/FAO/sxLr0_whe.tif")
oat <- raster("./inputs/FAO/sxLr0_oat.tif")
rye <- raster("./inputs/FAO/sxLr0_rye.tif")
barley <- raster("./inputs/FAO/sxLr0_brl.tif")

# We extract mean value at grid level
grid_FAO$FAO_wheat <- exact_extract(wheat, grid_FAO, fun = "mean")
grid_FAO$FAO_oat <- exact_extract(oat, grid_FAO, fun = "mean")
grid_FAO$FAO_rye <- exact_extract(rye, grid_FAO, fun = "mean")
grid_FAO$FAO_barley <- exact_extract(barley, grid_FAO, fun = "mean")

# We normalized using a 0 - 10000 threshold and not the maximum and min. Hence if a crop type range from 1000 to 8000 in europe, 1000 and 8000 would take value 0 and 1 respectively and be comparable to another crop with min and max 0-10000.
# Although all different crops range from 0 to 10000 expect for Rye (max 9999.694)

grid_FAO <- grid_FAO %>% mutate(FAO_wheat = (FAO_wheat - 0)/(10000 - 0), 
                                FAO_oat = (FAO_oat - 0)/(10000 - 0), 
                                FAO_rye = (FAO_rye - 0)/(10000 - 0), 
                                FAO_barley = (FAO_barley - 0)/(10000 - 0))

# We construct average suitability taking the mean of the 4 crops
grid_FAO <- grid_FAO %>% mutate(FAO_suit = (FAO_wheat + FAO_oat + FAO_rye + FAO_barley)/4)

# Remove temporary files
rm(barley, oat, rye, wheat)

# Quick epxloration of european agricultural suitability from FAO 4 major european crops
tm_shape(grid_FAO) + tm_polygons("FAO_suit", palette = "RdYlGn", breaks = seq(0, 1, by = 0.1)) + tm_shape(europe) + tm_borders()
save(grid_FAO, file = "./processed_data/grid_FAO.Rdata", overwrite = T)
```


Model calibration for functional forms TO UPDATE AND OUTSOURCE FUNCTIONAL FORMS
```{r}
# We load the gridded dataset processed in the main script 
grid_fitting <- grid_europe

load("./processed_data/grid_GDD.Rdata")
load("./processed_data/grid_AI.Rdata")
load("./processed_data/grid_HWSD.Rdata")
load("./processed_data/grid_FAO.Rdata")


# For the time varying parameters, we select the last 30 years since FAO gives suitability using climate data from 1971 - 2000
# Since first grid is 1500, grid 472 is 1971 and 501 is 2000
GDD_grid_71_00 <- grid_GDD[c(472:501)]
AI_index_grid_71_00 <- grid_AI[c(472:501)]

# We repeatedly left join to have the 30 years over each column so then we can do a rowmeans
# We start with GDD
grid_gdd_30_avg <- GDD_grid_71_00[[1]] %>% dplyr::select(gid, gdd)
for (i in 2:30){
  grid_gdd_30_avg <- left_join(grid_gdd_30_avg, GDD_grid_71_00[[i]] %>%
                                   st_drop_geometry() %>% dplyr::select(gid, gdd), by = "gid")
}

# Remove the geometry and grid id (order is maintained)
rows_gdd <- grid_gdd_30_avg %>% st_drop_geometry() %>%
  dplyr::select(-gid)
# We compute average GDD for each grid over the past 30 years using the rowmean 
rows_gdd$gdd_30 <- rowMeans(rows_gdd, na.rm = T)

# ai
grid_ai_30_avg <- AI_index_grid_71_00[[1]] %>% dplyr::select(gid, AI)
for (i in 2:30){
  grid_ai_30_avg <- left_join(grid_ai_30_avg, AI_index_grid_71_00[[i]] %>%
                                   st_drop_geometry() %>% dplyr::select(gid, AI), by = "gid")
}

rows_ai <- grid_ai_30_avg %>% st_drop_geometry() %>%
  dplyr::select(-gid)
rows_ai$ai_30 <- rowMeans(rows_ai, na.rm = T)


# We attach the soil ph and carbon content
grid_fitting <- left_join(grid_fitting, grid_HWSD %>% st_drop_geometry() %>% dplyr::select(gid, pH, carbon), by = "gid")

# We bring the average back to the grid
grid_fitting$gdd_30 <- rows_gdd$gdd_30
grid_fitting$ai_30 <- rows_ai$ai_30

grid_fitting <- left_join(grid_fitting, grid_FAO %>% st_drop_geometry() %>% dplyr::select(gid, FAO_suit), by = "gid")


# Remove temporary files
rm(grid_AI, AI_index_grid_71_00, grid_GDD, GDD_grid_71_00, grid_ai_30_avg, grid_gdd_30_avg, grid_HWSD, rows_ai, rows_gdd)

# We save the grid for technical validation exploration
 save(grid_fitting, file = "./processed_data/grid_fitting.Rdata", overwrite = T)
```



For the fitting, each element is looked separately to investigate its impact on suitability
Following ramankutty, we need to impose some filtering for other inputs:
For instance, when looking at GDD, we need to fix the other parameters so that we don't pick areas not suitable in the first place (very bad pH, carbon and moisture).

### GDD
```{r}
# For GDD, chosen points have 
# 4 < Csoil < 10
# 6 < pH < 8
# ai > 0.5
grid_fitting_gdd <- grid_fitting %>% filter(carbon > 4 & carbon < 10 & ai_30 > 0.5 & pH > 6 & pH < 8) %>% dplyr::select(gid, FAO_suit, gdd_30)

# We now construct bins of observations and take the max of each bins 

# We creat the bins
grid_fitting_gdd <- grid_fitting_gdd %>% mutate(points_bin = cut(gdd_30, breaks= 50))

# For each GDD bins, we take the average GDD value and associate it to the max suitability value (i.e. given this GDD range, what is the maximal yield we can get)
grid_fitting_gdd <- grid_fitting_gdd %>% group_by(points_bin) %>% mutate(gdd_bin = mean(gdd_30), 
                                                                         FAO_limit = max(FAO_suit)) %>% ungroup()


#important to sort along x values so that the plotting looks like one single line
# Lets first select and drop the geometry
dataset <- grid_fitting_gdd %>% st_drop_geometry() %>% dplyr::select(gdd_bin, FAO_limit)
dataset <- dataset[order(dataset$gdd_bin),] # Ordering


# Since we still have repeated bins and FAO_limit (all obs within each bins are an observations), we just take one per bins, hence using the unique function()
df_unique <- unique(dataset)
df_unique <- data.frame(x=df_unique$gdd_bin, y=df_unique$FAO_limit)

# Limit of fitting curve when data first reach best suitable conditions 1205.6464
df_unique <- df_unique %>% filter(x < 1205.6464)

# fitting code using the sigmoidal functional form with initial guess taking the one in Ramankutty
fitmodel <- nls(y~1/(1 + exp(a * (b-x))), start=list(a=.05,b=1100), data = df_unique)

# We define our sigmoidal function to have an idea of the fitting with the data that have been used
sigmoid <- function(params_gdd, x) {
  1 / (1 + exp(params_gdd[1] * ((params_gdd[2]) - x)))
}

# visualization code
# get the coefficients using the coef function
params_gdd <- coef(fitmodel)


# We plot the bin data with the sigmoidal curve and fitting parameters found previously
ggplot(data = dataset, aes(x = gdd_bin, y = FAO_limit)) + geom_point() +
  labs(y = "Suitability index (FAO)", x = "Growing degree days") +
  theme_minimal(base_family = "serif", base_size = 15) +
  stat_function(fun=function(x){(1/(1+exp(params_gdd[1]*(params_gdd[2]-x))))})
 ggsave(filename = "./figures/fit_GDD.png", bg = "white")
```

### AI
We repeat the procedure using the aridity index

```{r}
# For GDD, chosen points have 
# 4 < Csoil < 10
# 6 < pH < 8
# GDD > 1300
grid_fitting_ai <- grid_fitting %>% filter(carbon > 4 & carbon < 10 & gdd_30 > 1300 & pH > 6 & pH < 8) %>% dplyr::select(gid, FAO_suit, ai_30)

# We drop it for now 
grid_fitting_ai <- grid_fitting_ai %>% filter(!is.na(ai_30))

bins_ai <- (max(grid_fitting_ai$ai_30) - min(grid_fitting_ai$ai_30))/50

# We now construct bins of observations and take the max of each bins 

grid_fitting_ai <- grid_fitting_ai %>% mutate(points_bin = cut(ai_30, breaks = 50))

# For each AI bins, we take the average AI value and associate it to the max suitability value (i.e. given this AI range, what is the maximal yield we can get)
grid_fitting_ai <- grid_fitting_ai %>% group_by(points_bin) %>% mutate(ai_bin = mean(ai_30), 
                                                                         FAO_limit = max(FAO_suit)) %>% ungroup()


#important to sort along x values so that the plotting looks like one single line
# Lets first select and drop the geometry
dataset_ai <- grid_fitting_ai %>% st_drop_geometry() %>% dplyr::select(ai_bin, FAO_limit)
dataset_ai <- dataset_ai[order(dataset_ai$ai_bin),] # Ordering

# Reach close to 1 at 0.3859917

# Since we still have repeated bins and FAO_limit, we just take one per bins, hence using the unique function()
df_unique_ai <- unique(dataset_ai)
df_unique_ai <- data.frame(x=df_unique_ai$ai_bin, y=df_unique_ai$FAO_limit)

# The fact that we have y = 1 after 0.3859917 impose a limit curve and is not relevant for the computation of the parameters (doesn't change anything if we remove the filter)
df_unique_ai <- df_unique_ai %>% filter(x <= 0.3859917)

# fitting code
fitmodel_ai <- nls(y~1/(1 + exp(c * (d-x))), start=list(c=14,d=0.3), data = df_unique_ai)

# We define our sigmoid function to have an idea of the fitting with the data that have been used
sigmoid = function(params_ai, x) {
  1 / (1 + exp(params_ai[1] * ((params_ai[2]) - x)))
}
# visualization code
# get the coefficients using the coef function
params_ai=coef(fitmodel_ai)

# We plot the bin data with the sigmoidal curve and fitting parameters found previously
ggplot(data = dataset_ai, aes(x = ai_bin, y = FAO_limit)) + geom_point() +
  labs(y = "Suitability index (FAO)", x = "Aridity index") +
  theme_minimal(base_family = "serif", base_size = 15) +
  stat_function(fun=function(x){(1/(1+exp(params_ai[1]*(params_ai[2]-x))))})
 ggsave(filename = "./figures/fit_AI.png", bg = "white")
```


### Carbon 
```{r}
load("./processed_data/grid_fitting.Rdata")
# For carbon, chosen points have 
# 6 < pH < 8
# ai > 0.5
# GDD > 1300
grid_fitting_carbon <- grid_fitting %>% filter(ai_30 > 0.5 & pH > 6 & pH < 8 & gdd_30 > 1300) %>% dplyr::select(gid, FAO_suit, carbon)

# We now construct bins of observations and take the max of each bins 

grid_fitting_carbon <- grid_fitting_carbon %>% mutate(points_bin = cut(carbon, breaks=50))

# For each carbon bins, we take the average carbon value and associate it to the max suitability value (i.e. given this carbon range, what is the maximal yield we can get)
grid_fitting_carbon <- grid_fitting_carbon %>% group_by(points_bin) %>% mutate(carbon_bin = mean(carbon), 
                                                                         FAO_limit = max(FAO_suit)) %>% ungroup()

#important to sort along x values so that the plotting looks like one single line
# Lets first select and drop the geometry
dataset_carbon <- grid_fitting_carbon %>% st_drop_geometry() %>% dplyr::select(carbon_bin, FAO_limit)
dataset_carbon <- dataset_carbon[order(dataset_carbon$carbon_bin),] # Ordering

# Since we still have repeated bins and FAO_limit, we just take one per bins, hence using the unique function()
df_unique_carbon <- unique(dataset_carbon)
df_unique_carbon <- data.frame(x=df_unique_carbon$carbon_bin, y=df_unique_carbon$FAO_limit)

# fitting code
# Here we use the nlsLM function that finds convergence when nls gives "singular gradient"
fitmodel_carbon <- nlsLM(y ~ a/(1 + exp(b * (c-x))) * a/(1 + exp(d * (e-x))), start=list(a=3, b = 1.3, c = 3.4, d=-0.08, e = -27), data = df_unique_carbon)

# We define our sigmoid function to have an idea of the fitting with the data that have been used
double_sigmoid = function(params_carbon, x) {
  params_carbon[1] / (1 + exp(params_carbon[2] * ((params_carbon[3]) - x))) * params_carbon[1] / (1 + exp(params_carbon[4] * ((params_carbon[5]) - x)))
}


# Find the maximum value of the double sigmoidal function
max_value_carbon <- max(double_sigmoid(coef(fitmodel_carbon), df_unique_carbon))

# visualization code
# get the coefficients using the coef function
params_carbon=coef(fitmodel_carbon)

# We plot the bin data with the double sigmoidal curve and fitting parameters found previously
ggplot(data = dataset_carbon, aes(x = carbon_bin, y = FAO_limit)) + geom_point() +
  labs(y = "Suitability index (FAO)", x = "Soil carbon content") +
  theme_minimal(base_family = "serif", base_size = 15) +
  stat_function(fun=function(x){((params_carbon[1]/(1+exp(params_carbon[2]*(params_carbon[3]-x)))) * (params_carbon[1]/(1+exp(params_carbon[4]*(params_carbon[5]-x)))))/max_value_carbon})
  ggsave(filename = "./figures/fit_Csoil.png", bg = "white")
```

### pH 
```{r}
# For carbon, chosen points have 
# 4 < carbon < 10
# ai > 0.5
# GDD > 1300
grid_fitting_ph <- grid_fitting %>% filter(ai_30 > 0.5 & carbon > 4 & carbon < 10 & gdd_30 > 1300) %>% dplyr::select(gid, FAO_suit, pH)

# We now construct bins of observations and take the max of each bins
grid_fitting_ph <- grid_fitting_ph %>% mutate(points_bin = cut(pH, breaks= 50))

# For each pH bins, we take the average carbon value and associate it to the max suitability value (i.e. given this pH range, what is the maximal yield we can get)
grid_fitting_ph <- grid_fitting_ph %>% group_by(points_bin) %>% mutate(pH_bin = mean(pH), 
                                                                         FAO_limit = max(FAO_suit)) %>% ungroup()

#important to sort along x values so that the plotting looks like one single line
# Lets first select and drop the geometry
dataset_ph <- grid_fitting_ph %>% st_drop_geometry() %>% dplyr::select(pH_bin, FAO_limit)
dataset_ph <- dataset_ph[order(dataset_ph$pH_bin),] # Ordering


# We can see that we have 3 parts: 
# First the linear increase from 0 to approx 6.2, then the plateau until 7.2 and finally a linear decrease.
# Lets find the first plateau point and fit a linear relationship 
# First plateau it reached at 6.280128.


# Since we still have repeated bins and FAO_limit, we just take one per bins, hence using the unique function()
df_unique_ph <- unique(dataset_ph)
df_unique_ph <- data.frame(x=df_unique_ph$pH_bin, y=df_unique_ph$FAO_limit)

# fitting code for the first plateau
# Here we fit a linear model for pH to reach the first plateau
fitmodel_ph <- lm(y ~ x, data = df_unique_ph %>% filter(x <= 6.280128))

# We define our sigmoid function to have an idea of the fitting with the data that have been used
linear_model <- function(params_ph, x) {
params_ph[1] + params_ph[2]*x
}

# visualization code
# get the coefficients using the coef function
params_ph <- c(as.numeric(fitmodel_ph$coefficients[1]), as.numeric(fitmodel_ph$coefficients[2]))

# Hence, pH value reach plateau for x greater or equal to 
round((1-params_ph[1])/params_ph[2], digits = 2) #6.53, which correspond to estimates from ramankutty (6.5).

# Second plateau, pH will take value 1 until it starts to decrease again after 7.194691

# fitting code
# Here we fit a linear model for pH to reach the first plateau
fitmodel_ph_2 <- lm(y ~ x, data = df_unique_ph %>% filter(x > 7.206488))

# We define our sigmoid function to have an idea of the fitting with the data that have been used
linear_model_2 <- function(params_ph_2, x) {
params_ph_2[1] + params_ph_2[2]*x
}

# visualization code
# get the coefficients using the coef function
params_ph_2 <- c(as.numeric(fitmodel_ph_2$coefficients[1]), as.numeric(fitmodel_ph_2$coefficients[2]))

# Equal to 1 when x =
round((1 - params_ph_2[1])/params_ph_2[2], digits = 2) # 7.09

# Lets plot our 3 parts

# We plot the bin data with the double sigmoidal curve and fitting parameters found previously
ggplot(data = dataset_ph, aes(x = pH_bin, y = FAO_limit)) + geom_point() +
  labs(y = "Suitability index (FAO)", x = "Soil potential hydrogen") +
  theme_minimal(base_family = "serif", base_size = 15) +
  stat_function(fun=function(x){params_ph[1] + params_ph[2]*x}, xlim = c(min(dataset_ph$pH_bin), 6.53)) +
  stat_function(fun=function(x){1}, xlim = c(6.53, 7.09)) +
  stat_function(fun=function(x){params_ph_2[1] + params_ph_2[2]*x}, xlim = c(7.09, max(dataset_ph$pH_bin)))
 ggsave(filename = "./figures/fit_pHsoil.png", bg = "white")
```



## B) Applying the functional forms

### f(AI)

```{r}
load("./processed_data/grid_AI.Rdata")
# Computation of the f(AI)
for (i in 1:501){
  # We first rename the colnames so that it easier to apply the mutate in a loop 
  colnames(grid_AI[[i]]) <- c("gid", "AI", "geometry")
  
  # We create the f_ai following our parameters
  grid_AI[[i]] <- grid_AI[[i]] %>% mutate(f_ai = 1/(1+exp(as.numeric(params_ai[1])*(as.numeric(params_ai[2])-AI))))
}

 save(grid_AI, file = "./processed_data/grid_AI.Rdata", overwrite = T)
```


Carbon content functional forms
```{r}
load("./processed_data/grid_HWSD.Rdata")
# We compute the f(C), without forgetting to normalize the function dividing by max_carbon value
grid_HWSD <- grid_HWSD %>% mutate(f_carbon = ((as.numeric(params_carbon[1])/(1 + exp(as.numeric(params_carbon[2])*(as.numeric(params_carbon[3]) - carbon))))*(as.numeric(params_carbon[1])/(1 + exp(as.numeric(params_carbon[4])*(as.numeric(params_carbon[5]) - carbon))))/max_value_carbon))

grid_HWSD$f_carbon %>% summary()

grid_HWSD <- grid_HWSD %>% mutate(f_carbon =  ifelse(f_carbon > 1, 1, f_carbon))
```

### f(pH)

```{r, echo=F}
grid_HWSD$pH %>% summary()

# We create a dummy variable to indicate to which class the pH belong to
# Class 1 is the downward phases with high concentration of pH (alkaline conditions)
# Class 2 is the upward phase with low value of pH but increasing get better (acidic conditions)
# Class 3 are perfect conditions 
grid_HWSD <- grid_HWSD %>% mutate(class_1 = ifelse(pH > 7.2, 1, 0), 
                              class_2 = ifelse(pH < 6.3, 1, 0), 
                              class_3 = ifelse(pH <= 7.2 & pH >= 6.3, 1, 0))

# Apply the functional forms 
grid_HWSD <- grid_HWSD %>% mutate(f_ph = ifelse(class_1 == 1, as.numeric(params_ph_2[1])+as.numeric(params_ph_2[2])*pH, 
                                            ifelse(class_2 == 1, as.numeric(params_ph[1])+as.numeric(params_ph[2])*pH, 
                                                   ifelse(class_3 == 1, 1, 0))))

grid_HWSD$f_ph %>% summary()

#We bottom and top code 
grid_HWSD <- grid_HWSD %>% mutate(f_ph = ifelse(f_ph < 0, 0, 
                                            ifelse(f_ph > 1, 1, f_ph)))

grid_HWSD <- grid_HWSD %>% dplyr::select(-c(class_1, class_2, class_3))

 save(grid_HWSD, file = "./processed_data/grid_HWSD.Rdata", overwrite = T)
```

### f(GDD)

```{r, echo=F}
load("./processed_data/grid_GDD.Rdata")
for (i in 1:501){
  grid_GDD[[i]] <- grid_GDD[[i]] %>% mutate(f_gdd = 1/(1+exp(as.numeric(params_gdd[1])*(as.numeric(params_gdd[2])-gdd))))
}

 save(grid_GDD, file = "./processed_data/grid_GDD.Rdata", overwrite = T)
```


# Part 3: Creation of the index 

Putting everything together we can now build the measure of crop suitability:

```{r, echo=F}
load("./processed_data/grid_GDD.Rdata")
load("./processed_data/grid_AI.Rdata")
load("./processed_data/grid_HWSD.Rdata")

grid_suit <- rep(list(grid_europe), 501)
for (i in 1:501){
grid_suit[[i]]$f_gdd <- grid_GDD[[i]]$f_gdd
grid_suit[[i]]$f_ai <- grid_AI[[i]]$f_ai
grid_suit[[i]]$f_c <- grid_HWSD$f_carbon
grid_suit[[i]]$f_ph <- grid_HWSD$f_ph
grid_suit[[i]] <- grid_suit[[i]] %>% mutate(suit = f_gdd*f_ai*f_c*f_ph)  
}

save(grid_suit, file = "./processed_data/grid_suit.Rdata", overwrite = T)

```


Creation of the raster
```{r}
# We creat the extent raster with the extent of the project, resolution and CRS (EPSG:4326)
my_rast <- raster(extent(EXTproject), res = 0.5, crs = CRSproject)

# We create a list where we store the different raster
# We use the fasterize function which is way faster and produce similar results than the terra packages.
raster_suit <- rep(list(0), 501)


for (i in 1:501){
  raster_suit[[i]] <- fasterize::fasterize(grid_suit[[i]], my_rast, field = "suit", fun = "last")
}

# Stack all the layers together
raster_suit_brick <- raster::brick(raster_suit)

# Change names of raster layer
names_suit <- paste("suit", sep = "_", as.character(seq(1500, 2000, by = 1)))

names(raster_suit_brick) <- names_suit

# Change name of the dataset before saving
suit <- raster_suit_brick

 writeRaster(suit, filename = "./processed_data/suit.tif", bandorder = "BIL", overwrite = TRUE)   
```

